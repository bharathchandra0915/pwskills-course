{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f93d80",
   "metadata": {},
   "source": [
    "# Q1)\n",
    "### If our model is very sensitive to the train data i.e the model is well fitted and well trained, then it predicts inaccurately for an unseen new data point (poor performance for test data). This phenomenon of machine learning model to have a high accuracy with train dataset  and very low accuracy with test data is called overfitting. The major consequence is that this model performs very poor with real world data. It can be  mitigatated by regularization or by having a proper dataset splitting.\n",
    "### Underfitting is the phenomenon where model has low accuracy with both train and test dataset. It is barely useful model for real word applications. It can be mitigated by having more data points,  a proper training and by selecting appropriate features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e058b22",
   "metadata": {},
   "source": [
    "# Q2)\n",
    "### One commonly used technique to reduce overfitting is Regularization. Cross Validation is one more technique. We can remove some features while training so that the model would be less sensitive to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28a49b",
   "metadata": {},
   "source": [
    "# Q3)\n",
    "### When the model performs poorly with the train dataset i.e the model cannot capture the true relationships (between x and y of training data) properly then it is underfitted.  Underfitting occurs when the feature selection is poor or when the amount training data is not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ef43e",
   "metadata": {},
   "source": [
    "# Q4)\n",
    "### Bias is how accurately the model predicts for the training data. A model becomes low bias when there are less parameters used for training, which makes the model simple and underfits the train data. If there are more parameters used for training the model then model becomes more sensitivie, making it overfits the train data. Hence a proper number of features, parameters, amount of data for a good balance without overfitting and underfitting the data. This is called bias-variance tradeoff.\n",
    "### There is inverse relationship among these two. A lower bias model (meaning this fits the train data very well) has varying predictions for other unseen data points i.e higher variance. \n",
    "### A model will perform better if it has less bias and less variance. If bias is less then it may lead to overfitting, if bias is more it leads to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05602a9a",
   "metadata": {},
   "source": [
    "# Q5)\n",
    "### Overfitting and underfittng can be identified by comparing the accuracy and loss values of the model for validation and training data.\n",
    "### If accuracy with training data is high but with validation data its too low, then it is Overfitted model.\n",
    "### If accuracy with training data is only very low, then model is underfitted (validation score would be however low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c13de1",
   "metadata": {},
   "source": [
    "# Q6)\n",
    "###  Bias is term used for train dataset, variance is used for validation data.\n",
    "### Linear regression or any linear function model is an example of high bias, when used to fit the data that has the non-linear data. Decision trees, SVMs are example of high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c851b3",
   "metadata": {},
   "source": [
    "# Q7)\n",
    "### Regularization is one of the techniques used to reduce the overfitting of the model. \n",
    "### Here, a parameter (a small value) is added to the cost funtion. The aim of the model training is to reduce the cost funtion. But when this parameter is added or increased then, the value of the model features ($X_{i}$) will be reduced so as to maintain the whole cost function minimum. Hence by decreasing the parameter/feature values the curve will now be less sensitive to the data. This is how overfitting is reduced.\n",
    "### There are two types $L_{1}$ regularization and $L_{2}$ regularization. In  $L_{2}$ regularization technique weighted L2 norm of the weights is added, whereas in  $L_{1}$ regularization L1 norm or the absolute value of the weights are used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
